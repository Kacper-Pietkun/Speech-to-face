# General
Here is a list of the three models that are used in a Speech2Face pipeline. We based this project on the  [Speech2Face: Learning the Face Behind a Voice](https://arxiv.org/abs/1905.09773) paper and you can find the description of each model and its implementation below.

# Face Encoder
Architecture is based on a [Deep Face Recognition](https://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf) paper.
- **Goal:** Convert image of a face into a feature vector representation of that face
- **Model input:** Face image of size 224x224 (with 3 channels RGB)
- **Model output:** 4096-D feature vector of the face.

In this project we didn't implement and train this model ourselves, we used existing trained models from:
- VGG-face model from https://github.com/serengil/deepface (in our project it is called `VGGFace_serengil`)
- VGG-face (16) model from https://github.com/rcmalli/keras-vggface (in our project it is called `VGGFace16_rcmalli`)

However, these implementations are written in TensorFlow and we are using PyTorch, so we had to reimplement them in PyTroch (also we converted trained weights from TensorFlow to PyTorch - see `src/tensorflow_to_pytorch` directory)

# Voice Encoder
Architecture is based on a [Speech2Face: Learning the Face Behind a Voice](https://arxiv.org/abs/1905.09773) paper.
- **Goal:** Convert speech of a person (spectrogram generated from the audio file) into a feature vector representation of the face of that person (face embedding)
- **Model input:** spectrogram calculated from a 6 seconds audio file representing person's speech (using stft from librosa library)
- **Model output:** 4096-D feature vector of the face (voice encoder tries to learn to generate face embedding of the person given speech of that person).

Note: In this repository you can also use Audio Spectrogram Transformer as VoiceEncoder. It does not have its own file, because it is automatically downloaded from the Hugging Face repository in a training script.


# Face Decoder
Architecture is based on a [Synthesizing Normalized Faces from Facial Identity Features](https://arxiv.org/abs/1701.04851) paper.
- **Goal:** Convert a feature vector representation of the face into an actual image of the face described by that feature vector.
- **Model input:** 4096-D feature vector of the face (generated by either `FaceEncoder` or `VoiceEncoder`)
- **Model output:**
    1. Textures - image of the face of size 224x224 (with 3 channels RGB) 
    2. Landmark Locations - landmarks of the generated face - 1-D vector of size 144

 In our project we didn't implement differentialbe warping, which task would be to combine textures with generated landmarks
